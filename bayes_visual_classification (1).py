# -*- coding: utf-8 -*-
"""Bayes visual classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UNYlw5rrtp7uva933TPDiQ3i3o0YXjjl

# Librarries
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
import random as rnd
from skimage.transform import resize
from scipy.stats import norm
from google.colab import drive

drive.mount('/content/gdrive')

"""# Data Extraction"""

def unpickle(file):
    with open(file, 'rb') as f:
        dict = pickle.load(f, encoding="latin1")
    return dict

datadict_1 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_1')
datadict_2 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_2')
datadict_3 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_3')
datadict_4 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_4')
datadict_5 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_5')
X1 = datadict_1["data"]
X2 = datadict_2["data"]  
X3 = datadict_3["data"]  
X4 = datadict_4["data"]  
X5 = datadict_5["data"]  
Y1 = datadict_1["labels"] #10000 labels
Y2 = datadict_2["labels"]
Y3 = datadict_3["labels"]
Y4 = datadict_4["labels"]
Y5 = datadict_5["labels"]
#labeldict = unpickle('C:/Users/hamed/anaconda3/envs/dataml100/cifar-10-python/cifar-10-batches-py/batches.meta')
#label_names = labeldict["label_names"]
X = np.concatenate((X1, X2, X3, X4, X5), axis=0)
Y = np.concatenate((Y1, Y2, Y3, Y4, Y5), axis=0)
Y = np.array(Y)
X = X.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype("uint8")
t_datadic = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/test_batch')
Xte = t_datadic["data"] #nd-data
Xte = Xte.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype("uint8")
Yte = t_datadic["labels"] #10000 labels
Yte = np.array(Yte)

print(type(Y), type(X), X.shape, Yte.shape)
print(X[568,31,1,2])

"""# Accuracy assessment"""

def class_acc(pred,gt):
    subtract = pred - gt
    zero_elements = np.count_nonzero(subtract == 0)
    accuracy = zero_elements / len(subtract)
    return accuracy

"""# Good Bayesian Classifier"""

def cifar10_color(X,Xte):
  """ a function that reduce the image to Xp --> X (50000 * 32 * 32 *3) to Xp (50000 * 3)"""
  Xp = np.zeros((len(X),3))
  x_test = np.zeros((len(Xte),3))
  for i in range(0,len(X)):
    for j in range(0,3):
      Xp[i,:] = resize(X[i,...,j], (1,1))
  for i in range(0,len(Xte)):
    for j in range(0,3):
      x_test[i,:] = resize(Xte[i,...,j], (1,1))
  return Xp, x_test

Xp,x_test = cifar10_color(X,Xte)

print(Xp.shape)

print(Xp[1996,0],Xp.shape,x_test[9999,1],x_test.shape)

def cifar_10_naivebayes_learn(Xp, Y):
  """ compute the normal distribution parameters
  params: mu --> 10*3, sigma --> 10*3 ,p --> 10*1"""
  #Xpp = Xp[:,:,:,:,np.newaxis]
  i = 0
  mu = np.zeros((10,3))
  sigma = np.zeros((10,3))
  p = np.zeros((10,1))
  while i <= 9:
    ind = np.where(Y==i, )[0]
    #print(ind)
    subset = Xp[ind,]
    print(subset.shape)
    p[i] = np.count_nonzero(ind) / len(Xp)
    #for j in range(0,3):
    mu[i,] = np.mean(subset, axis = 0)
    sigma[i,] = np.var(subset, axis = 0)
    i+=1
    #w = np.split(Xp, Y, axis=0) 
  return mu, sigma, p

mu,sigma,p = cifar_10_naivebayes_learn(Xp, Y)
print(mu.shape,sigma.shape)
print(mu)

from numpy.core.fromnumeric import argmax
def cifar_classifier_naivebayes(x_test, mu, sigma, p):
  """ bayesian optimal class c for the sample x
  return: class c and accuracy"""
  i = 0
  classifier = np.zeros(len(x_test))
  for i in range(0,len(x_test)):
    probability = norm.pdf(x_test[i,], mu, sigma)
    c = np.prod(probability, axis = 1)
    pp = p.reshape(10,)
    #print(c.shape, p.shape)
    cc = np.multiply(pp,c)
    ind = cc.argmax()
    classifier[i,] = ind
  return classifier

classifier = cifar_classifier_naivebayes(x_test, mu, sigma, p)
acc = class_acc(classifier,Yte)
print("the accuracy of good model is", acc)
#print(probability[0,...], probability.shape, c[0,],c.shape)

"""# Better Bayesian Classifier"""

def cifar_10_bayes_learn(Xf,Y):
  """compute multivaraite normal distribution params (mu, sigma,p) for all classes 
    mu --> 10*3, sigma -->  10*3*3 priors p --> 10 * Numpy array"""
  i = 0
  mu = np.zeros((10,3))
  sigma = np.zeros((10,3,3))
  p = np.zeros((10,1))
  while i <= 9:
    ind = np.where(Y==i, )[0]
    subset = Xp[ind,]
    p[i] = np.count_nonzero(ind) / len(Xp)
    #for j in range(0,3):
    mu[i,] = np.mean(subset, axis = 0)
    sigma[i,] = np.cov(subset, rowvar = False)
    i+=1
    #w = np.split(Xp, Y, axis=0) 
  return mu, sigma, p

Xf = Xp
mu, sigma, p = cifar_10_bayes_learn(Xf, Y)
#print(mu.shape, sigma.shape)
print(sigma.shape, sigma)

print(Xf[[1100,200,300],...])

def cifar_10_classifier_bayes(x_test,mu,sigma,p):
  "compute probabilities and and classification accuracy"
  i = 0
  classifier = np.zeros(len(x_test))
  for i in range(0,len(x_test)):
    probability = norm.pdf(x_test[i,], mu, sigma)
    c = np.prod(probability, axis = 1)
    pp = p.reshape(10,)
    #print(c.shape, p.shape)
    cc = np.multiply(pp,c)
    ind = cc.argmax()
    classifier[i,] = ind
  return c

classifier = cifar_10_classifier_bayes(x_test, mu, sigma, p)
acc = class_acc(classifier,Yte)
print("the accuracy of good model is", acc)
#print(probability[0,...], probability.shape, c[0,],c.shape)

"""# Best Bayesian Classifier"""

def cifar10_2x2_color(X):
  """extend cifar10_color x --> 2*2*3 =12
  mu --> 12*1 , sigma --> 12*12
  compute performance for 1*1, 2*2, 4*4,...,32*32 images (only compute covariance)
  plot performance as a function of image size --> after some point the accuracy may collapse owing to less data points"""
  retutn Xp