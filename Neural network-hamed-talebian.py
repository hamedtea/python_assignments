# -*- coding: utf-8 -*-
"""Bayes visual classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UNYlw5rrtp7uva933TPDiQ3i3o0YXjjl

# Librarries
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
import random as rnd
from skimage.transform import resize
from scipy.stats import norm, multivariate_normal
from google.colab import drive

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras import layers
from keras import utils
#from sklearn.preprocessing import OneHotEncoder
from numpy.core.fromnumeric import argmax

drive.mount('/content/gdrive')

from sklearn.neighbors import KNeighborsClassifier
#from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

"""# Data Extraction"""

def unpickle(file):
    with open(file, 'rb') as f:
        dict = pickle.load(f, encoding="latin1")
    return dict

datadict_1 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_1')
datadict_2 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_2')
datadict_3 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_3')
datadict_4 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_4')
datadict_5 = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/data_batch_5')
X1 = datadict_1["data"]
X2 = datadict_2["data"]  
X3 = datadict_3["data"]  
X4 = datadict_4["data"]  
X5 = datadict_5["data"]  
Y1 = datadict_1["labels"] #10000 labels
Y2 = datadict_2["labels"]
Y3 = datadict_3["labels"]
Y4 = datadict_4["labels"]
Y5 = datadict_5["labels"]
#labeldict = unpickle('C:/Users/hamed/anaconda3/envs/dataml100/cifar-10-python/cifar-10-batches-py/batches.meta')
#label_names = labeldict["label_names"]
X_train = np.concatenate((X1, X2, X3, X4, X5), axis=0)
Y_train = np.concatenate((Y1, Y2, Y3, Y4, Y5), axis=0)
Y_train = np.array(Y_train)
X_train = X_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype("float32")/255.0
t_datadic = unpickle('/content/gdrive/MyDrive/Colab Notebooks/Datasets/cifar-10-batches-py/test_batch')
X_test = t_datadic["data"] #nd-data
X_test = X_test.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype("float32")/255.0
Y_test = t_datadic["labels"] #10000 labels
Y_test = np.array(Y_test)

"""# Accuracy assessment"""

def class_acc(pred,gt):
    subtract = pred - gt
    zero_elements = np.count_nonzero(subtract == 0)
    accuracy = zero_elements / len(subtract)
    return accuracy *100

"""# K-NN classifier"""

def k_NN_classifier(X_train, Y_train, X_test, Y_test):
  classifier = KNeighborsClassifier(n_neighbors=1)
  X_train_norm = np.reshape(X_train, (50000,3072))
  X_test_norm = np.reshape(X_test, (10000,3072))
  classifier.fit(X_train_norm, Y_train) 
  predictions = classifier.predict(X_test_norm)
  acc = accuracy_score(Y_test, predictions)
  return acc *100

"""# Good Bayesian Classifier"""

def Gaussian_naive_bayes(X_train, Y_train, X_test, Y_test ):
  gnb_model = GaussianNB()
  gnb_pred = gnb_model.fit(X_train.reshape(50000,3072), Y_train).predict(X_test.reshape(10000,3072))
  acc = accuracy_score(Y_test, gnb_pred)
  return acc

def cifar10_color(X_train,X_test):
  """ a function that reduce the image to Xp --> X (50000 * 32 * 32 *3) to Xp (50000 * 3)"""
  Xf = np.zeros((len(X_train),3))
  Xf_test = np.zeros((len(X_test),3))
  for i in range(0,len(X_test)):
    for j in range(0,3):
      Xf[i,j] = resize(X_train[i,...,j], (1,1))
  for i in range(0,len(X_test)):
    for j in range(0,3):
      Xf_test[i,j] = resize(X_test[i,...,j], (1,1))
  return Xf, Xf_test

a,b = cifar10_color(X_train,X_test)
print(a.shape, b.shape)

def cifar_10_naivebayes_learn(Xp, Y_train):
  """ compute the normal distribution parameters
  params: mu --> 10*3, sigma --> 10*3 ,p --> 10*1"""
  #Xpp = Xp[:,:,:,:,np.newaxis]
  i = 0
  mu = np.zeros((10,3))
  sigma = np.zeros((10,3))
  p = np.zeros((10,1))
  while i <= 9:
    ind = np.where(Y_train==i, )[0]
    #print(ind)
    subset = Xp[ind,]
    #print(subset.shape)
    p[i] = np.count_nonzero(ind) / len(Xp)
    #for j in range(0,3):
    mu[i,] = np.mean(subset, axis = 0)
    sigma[i,] = np.var(subset, axis = 0)
    i+=1
    #w = np.split(Xp, Y, axis=0) 
  return mu, sigma, p

mu,sigma,p = cifar_10_naivebayes_learn(Xp, Y_test)
print(mu.shape,sigma.shape, p.shape)
print(sigma)

def cifar_classifier_naivebayes(X_test, mu, sigma, p):
  """ bayesian optimal class c for the sample x
  return: class c and accuracy"""
  i = 0
  classifier = np.zeros(len(X_test))
  for i in range(0,len(X_test)):
    probability = norm.pdf(X_test[i,], mu, sigma)
    c = np.prod(probability, axis = 1)
    #pp = p.reshape(10,)
    #print(c.shape, p.shape)
    cc = np.multiply(p,c)
    ind = cc.argmax()
    classifier[i,] = ind
  return classifier

classifier = cifar_classifier_naivebayes(X_test, mu, sigma, p)
acc = class_acc(classifier,Y_test)
print("the accuracy of good model is", acc)
#print(probability[0,...], probability.shape, c[0,],c.shape)

"""# Better Bayesian Classifier"""

def cifar_10_bayes_learn(Xf, Y_train):
  """compute multivaraite normal distribution params (mu, sigma,p) for all classes 
    mu --> 10*3, sigma -->  10*3*3 priors p --> 10 * Numpy array"""
  i = 0
  mu = np.zeros((10,3))
  sigma = np.zeros((10,3,3))
  p = np.zeros((10,1))
  while i <= 9:
    ind = np.where(Y_train==i, )[0]
    subset = Xf[ind,]
    p[i] = np.count_nonzero(ind) / len(Xf)
    #for j in range(0,3):
    mu[i] = np.mean(subset, axis = 0)
    sigma[i] = np.cov(subset, rowvar = False)
    i+=1
  return mu, sigma, p

def cifar_10_classifier_bayes(Y_test):
  "compute probabilities and and classification accuracy"
  Xf, Xf_test = cifar10_color(X_train, X_test)
  mu, sigma, p = cifar_10_bayes_learn(Xf, Y_train)
  i = 0
  j= 0
  classifier = np.zeros(len(Xf_test))
  probability = np.zeros((10,3))
  for i in range(0,len(X_test)):
    for j in range(0,3):
      probability[j,] = multivariate_normal.pdf(Xf_test[i,], mu[j], sigma[j,...], allow_singular=True)
    ind = np.argmax(probability)
    classifier[i,] = ind
  acc = class_acc(classifier,Y_test)
  return acc

"""# Best Bayesian Classifier"""

def cifar10_2x2_color(X):
  """extend cifar10_color x --> 2*2*3 =12
  mu --> 12*1 , sigma --> 12*12
  compute performance for 1*1, 2*2, 4*4,...,32*32 images (only compute covariance)
  plot performance as a function of image size --> after some point the accuracy may collapse owing to less data points"""
  retutn Xp

"""# Simple Neural Network"""

def scheduler(epoch, lr):
  if epoch < 18:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

def neural_netwrok_simple(X_train, Y_train, X_test, Y_test):
  X_train_n = X_train.reshape((50000,3072))
  X_test_n = np.reshape(X_test, (10000,3072))
  #one hot encoder
  encoded_Y_train = keras.utils.to_categorical(Y_train, dtype ="float32")
  #adaptive learning rate operation
  callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
  #simple model generation
  simple_model = Sequential()
  simple_model.add(layers.Dense(5, input_dim=3072, activation='sigmoid'))
  simple_model.add(layers.Dense(10, activation='sigmoid'))
  #model indicators
  opt = keras.optimizers.SGD(learning_rate=0.01)
  #complie mode
  simple_model.compile(optimizer=opt, loss='mse', metrics=['mse'])
  #fit model
  simple_model.fit(X_train_n, encoded_Y_train, epochs=50, callbacks=[callback], verbose=0, shuffle = True)
  #model predictions
  pri=simple_model.predict(X_test_n)
  simple_NN_predictions = np.argmax(pri, axis = 1)
  #computing accuracy
  acc = class_acc(simple_NN_predictions,Y_test)
  return acc

def neural_netwrok(X_train, Y_train, X_test, Y_test):
  #encoded_Y_train = keras.utils.to_categorical(Y_train, dtype ="float32")
  model = Sequential( [
        tf.keras.layers.Input(shape= (32,32,3)),                                          
        tf.keras.layers.Conv2D(32, 5, padding = "valid", activation = "relu"),             
        tf.keras.layers.MaxPooling2D(pool_size = (2,2), padding = "valid"),                                  
        tf.keras.layers.Conv2D(64, 5, padding = "valid", activation = "relu"),                                                            
        tf.keras.layers.MaxPooling2D(pool_size = (2,2), padding = "valid"),                                  
        tf.keras.layers.Conv2D(128, 5, padding = "valid", activation = "relu"),                             
        tf.keras.layers.Flatten(),   
        tf.keras.layers.Dropout(0.1,noise_shape=None,seed=None),                                                     
        tf.keras.layers.Dense(128, activation = "relu"), 
        tf.keras.layers.Dense(64, activation = "relu"),
        tf.keras.layers.Dense(10,),                                                        
    
    ]
  )
  #compile model
  model.compile(
    #loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),     
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    #metrics =["accuracy"],
    loss='mse', 
    metrics=['sparse_categorical_accuracy'],                                            
    )
  #fit the moodel with training data and encoded one hot labels 
  training_history = model.fit(X_train, Y_train, batch_size = 32, epochs = 25, 
                              verbose = 1, validation_data = (X_test, Y_test), 
                               shuffle = True)
  #make predictions
  pre=model.predict(X_test)
  NN_predictions = np.argmax(pre, axis = 1)
  #evaluation
  test_loss, test_accuracy = model.evaluate(X_test, Y_test)
  #plot model
  tf.keras.utils.plot_model(
  model,
  to_file="model.png",
  show_shapes=False,
  show_dtype=False,
  show_layer_names=True,
  rankdir="TB",
  expand_nested=False,
  dpi=96,
  layer_range=None,
  show_layer_activations=False,)
  return model.summary, training_history, NN_predictions, test_loss, test_accuracy

def NN_model_summary(Y_test):
  model_summary, training_history, NN_predictions, test_loss, test_accuracy = neural_netwrok(X_train, Y_train, X_test, Y_test)
  acc = class_acc(NN_predictions, Y_test)
  print("the accuracy of better NN model is", acc)
  print('model loss equals to', test_loss)
  print('model accuracy equals to', test_accuracy )
  print(model_summary())
  #plot loss function
  plt.plot(training_history.history['loss'])
  plt.plot(training_history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epochs')
  plt.legend(['train_loss', 'value of loss'], loc = 'upper right')
  plt.show
  #plot accuracy function
  plt.figure()
  plt.plot(training_history.history['accuracy'])
  plt.plot(training_history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epochs')
  plt.legend(['train acuracy', 'value of accuracy'], loc = 'upper right')
  plt.show
  return

def main(): 
  k_NN_acc = k_NN_classifier(X_train, Y_train, X_test, Y_test)
  gb_acc = Gaussian_naive_bayes(X_train, Y_train, X_test, Y_test )
  bayes_acc = cifar_10_classifier_bayes(Y_test)
  simple_NN_acc = neural_netwrok_simple(X_train, Y_train, X_test, Y_test)
  print('the accuracy of Gaussian 1-NN classifier is', k_NN_acc, '%')
  print('the accuracy of Gaussian naive base classifier is', gb_acc, '%')
  print('the accuracy of better bayesian classifier with one pixel cifar10 dataset is', bayes_acc, '%')
  print('the accuracy of raw Neural Network with one layer and adaptive learning rate is', simple_NN_acc, '%')
  NN_model_summary(Y_test)
  return

if __name__ == "__main__":
    main()